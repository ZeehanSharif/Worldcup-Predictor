{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aaa7f46",
   "metadata": {},
   "source": [
    "# FinalCode — World Cup match forecasting (CS 418)\n",
    "\n",
    "This notebook runs end-to-end and generates all artifacts referenced by `FinalReport.ipynb`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23faa56d",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d8cbcc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT = C:\\CS418\\project-check-in-team-4\\worldcup-predictor\n",
      "Cleaned parquet = C:\\CS418\\project-check-in-team-4\\worldcup-predictor\\data\\processed\\matches_clean.parquet\n",
      "Cleaned CSV = C:\\CS418\\project-check-in-team-4\\worldcup-predictor\\data\\processed\\matches_clean.csv\n",
      "Figures dir = C:\\CS418\\project-check-in-team-4\\worldcup-predictor\\reports\\figures\n",
      "Metrics path = C:\\CS418\\project-check-in-team-4\\worldcup-predictor\\reports\\metrics_holdout_2022.json\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, log_loss\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "_CWD = Path.cwd().resolve()\n",
    "PROJECT_ROOT = None\n",
    "for p in [_CWD] + list(_CWD.parents):\n",
    "    if (p / \"src\" / \"config.py\").exists():\n",
    "        PROJECT_ROOT = p\n",
    "        break\n",
    "if PROJECT_ROOT is None:\n",
    "    raise FileNotFoundError(\"Could not locate project root (expected `src/config.py`).\")\n",
    "\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import src.config as cfg\n",
    "import src.cleaning as cleaning\n",
    "import src.elo as elo_mod\n",
    "import src.features as feat_mod\n",
    "\n",
    "RANDOM_STATE = getattr(cfg, \"TrainConfig\", object) and getattr(getattr(cfg, \"TrainConfig\", object), \"random_state\", 42)\n",
    "RANDOM_STATE = 42 if not isinstance(RANDOM_STATE, int) else RANDOM_STATE\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Canonical artifact paths (do not rely on cfg defining every constant)\n",
    "FIG_DIR = PROJECT_ROOT / \"reports\" / \"figures\"\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "METRICS_PATH = PROJECT_ROOT / \"reports\" / \"metrics_holdout_2022.json\"\n",
    "\n",
    "# Cleaned data paths\n",
    "MATCHES_CLEAN_PARQUET = getattr(cfg, \"MATCHES_CLEAN_PARQUET\", (PROJECT_ROOT / \"data\" / \"processed\" / \"matches_clean.parquet\"))\n",
    "DATA_PROCESSED_DIR = getattr(cfg, \"DATA_PROCESSED_DIR\", MATCHES_CLEAN_PARQUET.parent)\n",
    "MATCHES_CLEAN_CSV = DATA_PROCESSED_DIR / \"matches_clean.csv\"\n",
    "\n",
    "print(\"PROJECT_ROOT =\", PROJECT_ROOT)\n",
    "print(\"Cleaned parquet =\", MATCHES_CLEAN_PARQUET)\n",
    "print(\"Cleaned CSV =\", MATCHES_CLEAN_CSV)\n",
    "print(\"Figures dir =\", FIG_DIR)\n",
    "print(\"Metrics path =\", METRICS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cddec87",
   "metadata": {},
   "source": [
    "## 1. Data loading and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a57e0731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>team_home</th>\n",
       "      <th>team_away</th>\n",
       "      <th>score_home</th>\n",
       "      <th>score_away</th>\n",
       "      <th>match_result_1x2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1930</td>\n",
       "      <td>FRA</td>\n",
       "      <td>MEX</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>win</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1930</td>\n",
       "      <td>USA</td>\n",
       "      <td>BEL</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>win</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1930</td>\n",
       "      <td>ROU</td>\n",
       "      <td>PER</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>win</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year team_home team_away  score_home  score_away match_result_1x2\n",
       "0  1930       FRA       MEX           4           1              win\n",
       "1  1930       USA       BEL           3           0              win\n",
       "2  1930       ROU       PER           3           1              win"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load cleaned artifacts\n",
    "matches_clean = None\n",
    "\n",
    "# Prefer parquet if available\n",
    "if MATCHES_CLEAN_PARQUET.exists():\n",
    "    try:\n",
    "        matches_clean = pd.read_parquet(MATCHES_CLEAN_PARQUET)\n",
    "    except Exception:\n",
    "        matches_clean = None  # parquet engine may be unavailable in some environments\n",
    "\n",
    "# Fall back to CSV, then cleaning pipeline\n",
    "if matches_clean is None:\n",
    "    if MATCHES_CLEAN_CSV.exists():\n",
    "        matches_clean = pd.read_csv(MATCHES_CLEAN_CSV)\n",
    "    else:\n",
    "        matches_clean = cleaning.run_clean_and_save()\n",
    "\n",
    "# Ensure CSV exists for submission / compatibility\n",
    "DATA_PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "if not MATCHES_CLEAN_CSV.exists():\n",
    "    matches_clean.to_csv(MATCHES_CLEAN_CSV, index=False)\n",
    "\n",
    "# Lightweight sanity checks (avoid huge outputs)\n",
    "matches_clean[[\"year\",\"team_home\",\"team_away\",\"score_home\",\"score_away\",\"match_result_1x2\"]].head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aedfac56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n_matches_clean</td>\n",
       "      <td>755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>year_min</td>\n",
       "      <td>1930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>year_max</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n_unique_teams</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>label_counts</td>\n",
       "      <td>{'win': 410, 'loss': 201, 'draw': 144}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Metric                                   Value\n",
       "0  n_matches_clean                                     755\n",
       "1         year_min                                    1930\n",
       "2         year_max                                    2022\n",
       "3   n_unique_teams                                      76\n",
       "4     label_counts  {'win': 410, 'loss': 201, 'draw': 144}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compact data summary for the report\n",
    "data_summary = {\n",
    "    \"n_matches_clean\": int(len(matches_clean)),\n",
    "    \"year_min\": int(matches_clean[\"year\"].min()),\n",
    "    \"year_max\": int(matches_clean[\"year\"].max()),\n",
    "    \"n_unique_teams\": int(pd.unique(pd.concat([matches_clean[\"team_home\"], matches_clean[\"team_away\"]])).size),\n",
    "    \"label_counts\": matches_clean[\"match_result_1x2\"].value_counts().to_dict(),\n",
    "}\n",
    "pd.DataFrame({\"Metric\": list(data_summary.keys()), \"Value\": list(data_summary.values())})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74452aa9",
   "metadata": {},
   "source": [
    "## 2. Visualization — Elo trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b98107cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/CS418/project-check-in-team-4/worldcup-predictor/reports/figures/elo_trajectories.png')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build Elo features (chronological)\n",
    "feat_df = feat_mod.engineer_features(matches_clean)\n",
    "\n",
    "# Create a tidy team-level series of pre-match Elo for plotting\n",
    "home_part = feat_df[[\"date\",\"year\",\"team_home\",\"elo_home_pre\"]].rename(columns={\"team_home\":\"team\",\"elo_home_pre\":\"elo\"})\n",
    "away_part = feat_df[[\"date\",\"year\",\"team_away\",\"elo_away_pre\"]].rename(columns={\"team_away\":\"team\",\"elo_away_pre\":\"elo\"})\n",
    "elo_long = pd.concat([home_part, away_part], ignore_index=True).sort_values(\"date\")\n",
    "\n",
    "# Pick a small set of teams for the plot (top by match count)\n",
    "top_teams = (\n",
    "    elo_long[\"team\"].value_counts().head(6).index.tolist()\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for t in top_teams:\n",
    "    sub = elo_long[elo_long[\"team\"] == t]\n",
    "    # Downsample slightly if needed (still deterministic)\n",
    "    sub = sub.iloc[::max(1, len(sub)//200)]\n",
    "    plt.plot(sub[\"date\"], sub[\"elo\"], label=t)\n",
    "\n",
    "plt.title(\"Elo trajectories (pre-match) for frequently appearing teams\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Elo (pre-match)\")\n",
    "plt.legend(loc=\"best\", fontsize=8)\n",
    "plt.tight_layout()\n",
    "\n",
    "out_path = FIG_DIR / \"elo_trajectories.png\"\n",
    "plt.savefig(out_path, dpi=200)\n",
    "plt.close()\n",
    "\n",
    "out_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b080c7",
   "metadata": {},
   "source": [
    "## 3. ML/Stats — temporal holdout evaluation (2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b856b744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(694, 61, {'win': 32, 'loss': 20, 'draw': 9})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_YEAR = 2022\n",
    "LABELS_ORDER = [\"win\", \"draw\", \"loss\"]\n",
    "\n",
    "# Strict temporal split to avoid leakage\n",
    "train_df = feat_df[feat_df[\"year\"] < TEST_YEAR].copy()\n",
    "test_df  = feat_df[feat_df[\"year\"] == TEST_YEAR].copy()\n",
    "\n",
    "FEATURE_COLS = [\"elo_home_pre\", \"elo_away_pre\", \"elo_delta_pre\", \"is_knockout\"]\n",
    "TARGET_COL = \"match_result_1x2\"\n",
    "\n",
    "X_train = train_df[FEATURE_COLS]\n",
    "y_train = train_df[TARGET_COL].astype(str)\n",
    "\n",
    "X_test = test_df[FEATURE_COLS]\n",
    "y_test = test_df[TARGET_COL].astype(str)\n",
    "\n",
    "holdout_label_counts = y_test.value_counts().to_dict()\n",
    "\n",
    "len(train_df), len(test_df), holdout_label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "73bc6b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm: np.ndarray, labels: list[str], title: str, out_file: Path) -> None:\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    im = plt.imshow(cm, interpolation=\"nearest\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.xticks(range(len(labels)), labels, rotation=45, ha=\"right\")\n",
    "    plt.yticks(range(len(labels)), labels)\n",
    "\n",
    "    # annotate counts\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, str(int(cm[i, j])), ha=\"center\", va=\"center\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_file, dpi=200)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "39009c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\CS418\\project-check-in-team-4\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\CS418\\project-check-in-team-4\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:210: UserWarning: Labels passed were ['win', 'draw', 'loss']. But this function assumes labels are ordered lexicographically. Pass the ordered labels=['draw', 'loss', 'win'] and ensure that the columns of y_prob correspond to this ordering.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.4918032786885246,\n",
       " 'macro_f1': 0.2974036191974823,\n",
       " 'log_loss': 1.4816352099180283,\n",
       " 'classes': ['draw', 'loss', 'win']}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Technique #1: Multinomial Logistic Regression baseline (with scaling) using a Pipeline\n",
    "numeric_transformer = Pipeline(steps=[(\"scaler\", StandardScaler())])\n",
    "preproc_lr = ColumnTransformer(\n",
    "    transformers=[(\"num\", numeric_transformer, FEATURE_COLS)],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "model1 = Pipeline(\n",
    "    steps=[\n",
    "        (\"prep\", preproc_lr),\n",
    "        (\"clf\", LogisticRegression(max_iter=1000, random_state=RANDOM_STATE, multi_class=\"multinomial\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model1.fit(X_train, y_train)\n",
    "\n",
    "pred1 = model1.predict(X_test)\n",
    "proba1 = model1.predict_proba(X_test)\n",
    "classes1 = list(model1.named_steps[\"clf\"].classes_)\n",
    "\n",
    "# Align probabilities to LABELS_ORDER for log loss\n",
    "proba1_df = pd.DataFrame(proba1, columns=classes1)[LABELS_ORDER].to_numpy()\n",
    "\n",
    "res_lr = {\n",
    "    \"accuracy\": float(accuracy_score(y_test, pred1)),\n",
    "    \"macro_f1\": float(f1_score(y_test, pred1, average=\"macro\", labels=LABELS_ORDER)),\n",
    "    \"log_loss\": float(log_loss(y_test, proba1_df, labels=LABELS_ORDER)),\n",
    "    \"classes\": classes1,\n",
    "}\n",
    "cm1 = confusion_matrix(y_test, pred1, labels=LABELS_ORDER)\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    cm1, LABELS_ORDER,\n",
    "    title=\"Confusion matrix — Model 1 (LogReg) on 2022 holdout\",\n",
    "    out_file=FIG_DIR / \"cm_model1_2022.png\"\n",
    ")\n",
    "\n",
    "res_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6908ac63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\CS418\\project-check-in-team-4\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:210: UserWarning: Labels passed were ['win', 'draw', 'loss']. But this function assumes labels are ordered lexicographically. Pass the ordered labels=['draw', 'loss', 'win'] and ensure that the columns of y_prob correspond to this ordering.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5081967213114754,\n",
       " 'macro_f1': 0.4291486291486291,\n",
       " 'log_loss': 1.6192756076819315,\n",
       " 'classes': ['draw', 'loss', 'win']}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Technique #2: Random Forest (non-linear model) using a Pipeline (no scaling needed)\n",
    "preproc_rf = ColumnTransformer(\n",
    "    transformers=[(\"num\", \"passthrough\", FEATURE_COLS)],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "model2 = Pipeline(\n",
    "    steps=[\n",
    "        (\"prep\", preproc_rf),\n",
    "        (\"clf\", RandomForestClassifier(\n",
    "            n_estimators=300,\n",
    "            random_state=RANDOM_STATE,\n",
    "            class_weight=None,\n",
    "            min_samples_leaf=2,\n",
    "        )),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "pred2 = model2.predict(X_test)\n",
    "proba2 = model2.predict_proba(X_test)\n",
    "classes2 = list(model2.named_steps[\"clf\"].classes_)\n",
    "\n",
    "proba2_df = pd.DataFrame(proba2, columns=classes2)[LABELS_ORDER].to_numpy()\n",
    "\n",
    "res_rf = {\n",
    "    \"accuracy\": float(accuracy_score(y_test, pred2)),\n",
    "    \"macro_f1\": float(f1_score(y_test, pred2, average=\"macro\", labels=LABELS_ORDER)),\n",
    "    \"log_loss\": float(log_loss(y_test, proba2_df, labels=LABELS_ORDER)),\n",
    "    \"classes\": classes2,\n",
    "}\n",
    "cm2 = confusion_matrix(y_test, pred2, labels=LABELS_ORDER)\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    cm2, LABELS_ORDER,\n",
    "    title=\"Confusion matrix — Model 2 (RF) on 2022 holdout\",\n",
    "    out_file=FIG_DIR / \"cm_model2_2022.png\"\n",
    ")\n",
    "\n",
    "res_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee5e0dd",
   "metadata": {},
   "source": [
    "## 4. Additional work — permutation importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "38bc9a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance_mean</th>\n",
       "      <th>importance_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>elo_home_pre</td>\n",
       "      <td>0.125608</td>\n",
       "      <td>0.032628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>elo_delta_pre</td>\n",
       "      <td>0.056550</td>\n",
       "      <td>0.039081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is_knockout</td>\n",
       "      <td>0.015635</td>\n",
       "      <td>0.036389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>elo_away_pre</td>\n",
       "      <td>0.013950</td>\n",
       "      <td>0.047102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         feature  importance_mean  importance_std\n",
       "0   elo_home_pre         0.125608        0.032628\n",
       "2  elo_delta_pre         0.056550        0.039081\n",
       "3    is_knockout         0.015635        0.036389\n",
       "1   elo_away_pre         0.013950        0.047102"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extra deliverable: permutation importance on the 2022 holdout (macro-F1 scoring)\n",
    "# We'll compute this on the Random Forest pipeline (model2).\n",
    "perm = permutation_importance(\n",
    "    model2,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    scoring=\"f1_macro\",\n",
    "    n_repeats=15,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=1,\n",
    ")\n",
    "\n",
    "imp = pd.DataFrame(\n",
    "    {\"feature\": FEATURE_COLS, \"importance_mean\": perm.importances_mean, \"importance_std\": perm.importances_std}\n",
    ").sort_values(\"importance_mean\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.bar(imp[\"feature\"], imp[\"importance_mean\"], yerr=imp[\"importance_std\"])\n",
    "plt.title(\"Permutation importance (macro-F1) — 2022 holdout\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Mean decrease in macro-F1\")\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(FIG_DIR / \"perm_importance.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "imp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff02a16c",
   "metadata": {},
   "source": [
    "## 5. Save metrics artifact for the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "094089ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/CS418/project-check-in-team-4/worldcup-predictor/reports/metrics_holdout_2022.json')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = {\n",
    "    \"data_summary\": {\n",
    "        **data_summary,\n",
    "        \"holdout_year\": int(TEST_YEAR),\n",
    "        \"holdout_n_matches\": int(len(y_test)),\n",
    "        \"holdout_label_counts\": holdout_label_counts,\n",
    "    },\n",
    "    \"models\": {\n",
    "        \"model1_logreg\": {\n",
    "            \"name\": \"Multinomial Logistic Regression\",\n",
    "            **res_lr,\n",
    "            \"confusion_matrix_labels\": LABELS_ORDER,\n",
    "            \"confusion_matrix\": cm1.tolist(),\n",
    "        },\n",
    "        \"model2_random_forest\": {\n",
    "            \"name\": \"Random Forest\",\n",
    "            **res_rf,\n",
    "            \"confusion_matrix_labels\": LABELS_ORDER,\n",
    "            \"confusion_matrix\": cm2.tolist(),\n",
    "        },\n",
    "    },\n",
    "    \"artifacts\": {\n",
    "        \"fig_dir\": str(FIG_DIR.relative_to(PROJECT_ROOT)),\n",
    "        \"metrics_path\": str(METRICS_PATH.relative_to(PROJECT_ROOT)),\n",
    "        \"figures\": {\n",
    "            \"elo_trajectories\": \"reports/figures/elo_trajectories.png\",\n",
    "            \"cm_model1_2022\": \"reports/figures/cm_model1_2022.png\",\n",
    "            \"cm_model2_2022\": \"reports/figures/cm_model2_2022.png\",\n",
    "            \"perm_importance\": \"reports/figures/perm_importance.png\",\n",
    "        },\n",
    "        \"cleaned_csv\": str(MATCHES_CLEAN_CSV.relative_to(PROJECT_ROOT)),\n",
    "    },\n",
    "}\n",
    "\n",
    "METRICS_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "METRICS_PATH.write_text(json.dumps(metrics, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "METRICS_PATH"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
